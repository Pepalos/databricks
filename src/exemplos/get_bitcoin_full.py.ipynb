{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b2ad658f-582a-4d1a-b73e-7a8b57f93738",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC # üí∞ Extra√ß√£o de Dados Bitcoin - API Coinbase\n",
    "# MAGIC\n",
    "# MAGIC Este notebook demonstra como:\n",
    "# MAGIC - Extrair dados da API da Coinbase\n",
    "# MAGIC - Converter USD para BRL usando API de economia\n",
    "# MAGIC - Salvar dados em JSON, CSV e Parquet usando PySpark\n",
    "# MAGIC - Salvar como **Delta Table** no Databricks\n",
    "# MAGIC - Converter Delta Table para DataFrame\n",
    "# MAGIC - Trabalhar com **Unity Catalog** (Catalog, Schema, Volume)\n",
    "# MAGIC\n",
    "# MAGIC ---\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 1. Importando Bibliotecas Necess√°rias\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# No Databricks, o Spark j√° est√° dispon√≠vel como 'spark', n√£o precisa criar SparkSession\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 2. Extraindo e Transformando Dados\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def extrair_dados_bitcoin():\n",
    "    \"\"\"Extrai o JSON completo da API da Coinbase.\"\"\"\n",
    "    url = 'https://api.coinbase.com/v2/prices/spot'\n",
    "    resposta = requests.get(url)\n",
    "    return resposta.json()\n",
    "\n",
    "def extrair_cotacao_usd_brl():\n",
    "    \"\"\"Extrai a cota√ß√£o USD-BRL da API CurrencyFreaks.\"\"\"\n",
    "    api_key = 'bf8ddcbf744d4e7897e836941c32e39f'\n",
    "    url = f'https://api.currencyfreaks.com/v2.0/rates/latest?apikey={api_key}'\n",
    "    resposta = requests.get(url)\n",
    "    return resposta.json()\n",
    "\n",
    "def tratar_dados_bitcoin(dados_json, taxa_usd_brl):\n",
    "    \"\"\"Transforma os dados brutos da API, renomeia colunas, adiciona timestamp e converte para BRL.\"\"\"\n",
    "    valor_usd = float(dados_json['data']['amount'])\n",
    "    criptomoeda = dados_json['data']['base']\n",
    "    moeda_original = dados_json['data']['currency']\n",
    "    \n",
    "    # Convertendo de USD para BRL\n",
    "    valor_brl = valor_usd * taxa_usd_brl\n",
    "    \n",
    "    # Adicionando timestamp como datetime object\n",
    "    timestamp = datetime.now()\n",
    "    \n",
    "    dados_tratados = [{\n",
    "        \"valor_usd\": valor_usd,\n",
    "        \"valor_brl\": valor_brl,\n",
    "        \"criptomoeda\": criptomoeda,\n",
    "        \"moeda_original\": moeda_original,\n",
    "        \"taxa_conversao_usd_brl\": taxa_usd_brl,\n",
    "        \"timestamp\": timestamp\n",
    "    }]\n",
    "    \n",
    "    return dados_tratados\n",
    "\n",
    "# Extraindo dados\n",
    "Dados_bitcoin = extrair_dados_bitcoin()\n",
    "dados_cotacao = extrair_cotacao_usd_brl()\n",
    "\n",
    "# Extraindo a taxa de convers√£o USD-BRL\n",
    "taxa_usd_brl = float(dados_cotacao['rates']['BRL'])\n",
    "\n",
    "# Tratando os dados e convertendo para BRL\n",
    "dados_bitcoin_tratado = tratar_dados_bitcoin(Dados_bitcoin, taxa_usd_brl)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 3. Configurando Unity Catalog\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %sql\n",
    "# MAGIC CREATE CATALOG IF NOT EXISTS pipeline_api_bitcoin\n",
    "# MAGIC COMMENT 'Cat√°logo de demonstra√ß√£o criado para o workshop de pipeline_api_bitcoin';\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %sql\n",
    "# MAGIC CREATE SCHEMA IF NOT EXISTS pipeline_api_bitcoin.lakehouse\n",
    "# MAGIC COMMENT 'Schema Lakehouse para salvar dados processados';\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %sql\n",
    "# MAGIC CREATE VOLUME IF NOT EXISTS pipeline_api_bitcoin.lakehouse.raw_files\n",
    "# MAGIC COMMENT 'Volume para arquivos brutos de ingest√£o inicial';\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %sql\n",
    "# MAGIC CREATE SCHEMA IF NOT EXISTS pipeline_api_bitcoin.bitcoin_data\n",
    "# MAGIC COMMENT 'Schema para armazenar dados de Bitcoin processados';\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 4. Criando DataFrame Pandas\n",
    "# MAGIC\n",
    "# MAGIC Vamos criar um DataFrame Pandas para salvar os arquivos de forma simples.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Criar DataFrame Pandas a partir dos dados tratados\n",
    "df_pandas = pd.DataFrame(dados_bitcoin_tratado)\n",
    "\n",
    "# Mostrar dados\n",
    "print(df_pandas)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 5. Salvando em JSON usando Pandas\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Pega o timestamp do pr√≥prio evento\n",
    "event_ts = dados_bitcoin_tratado[0][\"timestamp\"]\n",
    "\n",
    "# Converte para formato seguro para nome de arquivo\n",
    "ts = event_ts.strftime(\"%Y%m%d_%H%M%S_%f\")\n",
    "\n",
    "# Caminho do arquivo JSON\n",
    "json_file = f\"/Volumes/pipeline_api_bitcoin/lakehouse/raw_files/bitcoin_{ts}.json\"\n",
    "\n",
    "# Salvar como JSON usando Pandas\n",
    "df_pandas.to_json(json_file, orient='records', date_format='iso', indent=2)\n",
    "print(f\"‚úÖ Arquivo JSON salvo: {json_file}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 6. Salvando como CSV usando PySpark\n",
    "# MAGIC\n",
    "# MAGIC ### üìÑ O que √© CSV?\n",
    "# MAGIC\n",
    "# MAGIC **CSV (Comma-Separated Values)** √© um formato de arquivo **texto** simples e universal:\n",
    "# MAGIC\n",
    "# MAGIC **Caracter√≠sticas T√©cnicas:**\n",
    "# MAGIC - ‚úÖ **Formato texto**: Arquivo leg√≠vel por humanos (pode abrir no Bloco de Notas)\n",
    "# MAGIC - ‚úÖ **Row-based**: Armazena dados por linha (otimizado para transa√ß√µes OLTP)\n",
    "# MAGIC - ‚úÖ **Delimitado por v√≠rgulas**: Valores separados por v√≠rgula (ou ponto-e-v√≠rgula)\n",
    "# MAGIC - ‚úÖ **Universal**: Suportado por praticamente todas as ferramentas (Excel, Python, R, SQL, etc.)\n",
    "# MAGIC - ‚úÖ **Simples**: F√°cil de entender, debugar e inspecionar manualmente\n",
    "# MAGIC\n",
    "# MAGIC **Vantagens:**\n",
    "# MAGIC - ‚úÖ **Legibilidade**: Pode ser aberto no Excel, Bloco de Notas, Google Sheets\n",
    "# MAGIC - ‚úÖ **Compatibilidade**: Funciona com qualquer ferramenta de dados\n",
    "# MAGIC - ‚úÖ **Debugging**: F√°cil de inspecionar e corrigir manualmente\n",
    "# MAGIC - ‚úÖ **Portabilidade**: Pode ser compartilhado facilmente\n",
    "# MAGIC\n",
    "# MAGIC **Desvantagens:**\n",
    "# MAGIC - ‚ùå **Maior tamanho**: Arquivos maiores que Parquet (sem compress√£o nativa)\n",
    "# MAGIC - ‚ùå **Mais lento**: Leitura e escrita mais lentas em grandes volumes\n",
    "# MAGIC - ‚ùå **Sem schema**: N√£o preserva tipos de dados automaticamente (tudo √© texto)\n",
    "# MAGIC - ‚ùå **Sem compress√£o**: Arquivos ocupam mais espa√ßo em disco\n",
    "# MAGIC - ‚ùå **Limita√ß√µes**: N√£o suporta tipos complexos (arrays, objetos aninhados)\n",
    "# MAGIC\n",
    "# MAGIC **Quando usar CSV?**\n",
    "# MAGIC - Dados pequenos ou m√©dios (< 100MB)\n",
    "# MAGIC - Quando precisa ser leg√≠vel por humanos\n",
    "# MAGIC - Integra√ß√£o com Excel ou ferramentas de neg√≥cio\n",
    "# MAGIC - Debugging e inspe√ß√£o manual dos dados\n",
    "# MAGIC - Exporta√ß√£o para sistemas que n√£o suportam Parquet\n",
    "# MAGIC - Prototipagem r√°pida\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Caminho do arquivo CSV\n",
    "csv_file = f\"/Volumes/pipeline_api_bitcoin/lakehouse/raw_files/bitcoin_{ts}.csv\"\n",
    "\n",
    "# Salvar como CSV usando Pandas\n",
    "df_pandas.to_csv(csv_file, index=False)\n",
    "print(f\"‚úÖ Arquivo CSV salvo: {csv_file}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 7. Salvando como Parquet usando PySpark\n",
    "# MAGIC\n",
    "# MAGIC ### üìä O que √© Parquet?\n",
    "# MAGIC\n",
    "# MAGIC **Parquet** √© um formato de arquivo **bin√°rio e columnar** otimizado para Big Data:\n",
    "# MAGIC\n",
    "# MAGIC **Caracter√≠sticas T√©cnicas:**\n",
    "# MAGIC - ‚úÖ **Formato bin√°rio**: Arquivo n√£o leg√≠vel por humanos (otimizado para m√°quinas)\n",
    "# MAGIC - ‚úÖ **Columnar**: Armazena dados por coluna, n√£o por linha (otimizado para analytics OLAP)\n",
    "# MAGIC - ‚úÖ **Compress√£o nativa**: Usa algoritmos de compress√£o eficientes (Snappy, Gzip, LZ4)\n",
    "# MAGIC - ‚úÖ **Schema embutido**: Mant√©m informa√ß√µes sobre tipos de dados automaticamente\n",
    "# MAGIC - ‚úÖ **Predicate pushdown**: Permite ler apenas colunas necess√°rias\n",
    "# MAGIC - ‚úÖ **Estat√≠sticas**: Inclui estat√≠sticas (min, max, null count) por coluna\n",
    "# MAGIC\n",
    "# MAGIC **Vantagens:**\n",
    "# MAGIC - ‚úÖ **Compress√£o**: Arquivos muito menores que CSV (at√© 90% de economia)\n",
    "# MAGIC - ‚úÖ **Performance**: Leitura r√°pida, especialmente para consultas anal√≠ticas\n",
    "# MAGIC - ‚úÖ **Big Data**: Ideal para processar grandes volumes de dados (terabytes/petabytes)\n",
    "# MAGIC - ‚úÖ **Schema**: Preserva tipos de dados (int, float, string, date, etc.)\n",
    "# MAGIC - ‚úÖ **Columnar**: Consultas que precisam de poucas colunas s√£o muito r√°pidas\n",
    "# MAGIC - ‚úÖ **Efici√™ncia**: Menor uso de I/O e mem√≥ria\n",
    "# MAGIC\n",
    "# MAGIC **Desvantagens:**\n",
    "# MAGIC - ‚ùå **N√£o √© leg√≠vel por humanos**: Precisa de ferramentas especiais (Pandas, Spark, etc.)\n",
    "# MAGIC - ‚ùå **Overhead**: Para dados muito pequenos, o overhead pode n√£o valer a pena\n",
    "# MAGIC - ‚ùå **Escrita mais lenta**: A compress√£o e organiza√ß√£o columnar tornam a escrita mais lenta\n",
    "# MAGIC\n",
    "# MAGIC **Quando usar Parquet?**\n",
    "# MAGIC - Grandes volumes de dados (> 100MB)\n",
    "# MAGIC - Data Lakes e Data Warehouses\n",
    "# MAGIC - Quando performance √© cr√≠tica\n",
    "# MAGIC - An√°lises anal√≠ticas (OLAP)\n",
    "# MAGIC - Processamento com Spark/PySpark\n",
    "# MAGIC - Quando economia de espa√ßo √© importante\n",
    "# MAGIC - Consultas que acessam poucas colunas de muitas linhas\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Caminho do arquivo Parquet\n",
    "parquet_file = f\"/Volumes/pipeline_api_bitcoin/lakehouse/raw_files/bitcoin_{ts}.parquet\"\n",
    "\n",
    "# Salvar como Parquet usando Pandas\n",
    "df_pandas.to_parquet(parquet_file, index=False)\n",
    "print(f\"‚úÖ Arquivo Parquet salvo: {parquet_file}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 8. Convertendo Pandas para PySpark e Salvando como Delta Table\n",
    "# MAGIC\n",
    "# MAGIC Agora vamos converter o DataFrame Pandas para PySpark para salvar no Delta Table.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Converter DataFrame Pandas para PySpark\n",
    "# type: ignore - spark est√° dispon√≠vel no ambiente Databricks\n",
    "df_spark = spark.createDataFrame(df_pandas)  # noqa: F821\n",
    "\n",
    "# Mostrar schema\n",
    "df_spark.printSchema()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 9. Salvando como Delta Table\n",
    "# MAGIC\n",
    "# MAGIC ### üéØ O que √© Delta Table?\n",
    "# MAGIC\n",
    "# MAGIC **Delta Lake** √© uma camada de armazenamento open-source que traz **ACID transactions** e **time travel** para Data Lakes. Delta Tables s√£o tabelas que usam o formato Delta.\n",
    "# MAGIC\n",
    "# MAGIC **Caracter√≠sticas T√©cnicas:**\n",
    "# MAGIC - ‚úÖ **ACID Transactions**: Garante consist√™ncia dos dados (Atomicity, Consistency, Isolation, Durability)\n",
    "# MAGIC - ‚úÖ **Time Travel**: Permite acessar vers√µes anteriores dos dados\n",
    "# MAGIC - ‚úÖ **Schema Enforcement**: Valida e garante que os dados seguem o schema definido\n",
    "# MAGIC - ‚úÖ **Upsert/Merge**: Suporta opera√ß√µes de atualiza√ß√£o e merge eficientes\n",
    "# MAGIC - ‚úÖ **Baseado em Parquet**: Usa Parquet como formato f√≠sico (herda todas as vantagens)\n",
    "# MAGIC - ‚úÖ **Transaction Log**: Mant√©m um log de todas as transa√ß√µes (Delta Log)\n",
    "# MAGIC - ‚úÖ **Optimize & Z-Order**: Ferramentas para otimizar performance\n",
    "# MAGIC\n",
    "# MAGIC **Vantagens sobre Parquet simples:**\n",
    "# MAGIC - ‚úÖ **Transa√ß√µes ACID**: M√∫ltiplas escritas simult√¢neas sem corrup√ß√£o\n",
    "# MAGIC - ‚úÖ **Time Travel**: Acesse vers√µes hist√≥ricas dos dados\n",
    "# MAGIC - ‚úÖ **Schema Evolution**: Permite evoluir o schema ao longo do tempo\n",
    "# MAGIC - ‚úÖ **Delete/Update**: Suporta opera√ß√µes de atualiza√ß√£o e dele√ß√£o\n",
    "# MAGIC - ‚úÖ **Auditoria**: Hist√≥rico completo de mudan√ßas\n",
    "# MAGIC - ‚úÖ **Performance**: Otimiza√ß√µes autom√°ticas (compacta√ß√£o, indexa√ß√£o)\n",
    "# MAGIC\n",
    "# MAGIC **Quando usar Delta Table?**\n",
    "# MAGIC - Quando precisa de transa√ß√µes ACID\n",
    "# MAGIC - Quando precisa de time travel (auditoria, rollback)\n",
    "# MAGIC - Quando precisa fazer updates/deletes em dados hist√≥ricos\n",
    "# MAGIC - Data Warehouses modernos (Lakehouse)\n",
    "# MAGIC - Pipelines que precisam de garantias de consist√™ncia\n",
    "# MAGIC - Quando m√∫ltiplos processos escrevem na mesma tabela\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Caminho da tabela Delta no Unity Catalog (schema bitcoin_data)\n",
    "delta_table_path = \"pipeline_api_bitcoin.bitcoin_data.bitcoin_data\"\n",
    "\n",
    "# Salvar como Delta Table usando o DataFrame PySpark (modo append se a tabela j√° existir)\n",
    "df_spark.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .saveAsTable(delta_table_path)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 10. Convertendo Delta Table para DataFrame\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Ler Delta Table como Spark DataFrame\n",
    "# type: ignore - spark est√° dispon√≠vel no ambiente Databricks\n",
    "df_delta = spark.read.table(delta_table_path)  # noqa: F821\n",
    "\n",
    "# Mostrar schema\n",
    "df_delta.printSchema()\n",
    "\n",
    "# Mostrar dados\n",
    "df_delta.show(truncate=False)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### Consultando Delta Table com SQL\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %sql\n",
    "# MAGIC SELECT * FROM pipeline_api_bitcoin.bitcoin_data.bitcoin_data\n",
    "# MAGIC ORDER BY timestamp DESC\n",
    "# MAGIC LIMIT 10\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### Verificando hist√≥rico da Delta Table (Time Travel)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %sql\n",
    "# MAGIC DESCRIBE HISTORY pipeline_api_bitcoin.bitcoin_data.bitcoin_data\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 11. Resumo do Pipeline\n",
    "# MAGIC\n",
    "# MAGIC Este pipeline completo realiza:\n",
    "# MAGIC\n",
    "# MAGIC 1. ‚úÖ **Extra√ß√£o**: Busca dados da API Coinbase e cota√ß√£o USD-BRL\n",
    "# MAGIC 2. ‚úÖ **Transforma√ß√£o**: Trata dados, converte para BRL e adiciona timestamp\n",
    "# MAGIC 3. ‚úÖ **Infraestrutura**: Cria Catalog e Schema Lakehouse no Unity Catalog\n",
    "# MAGIC 4. ‚úÖ **Carga em m√∫ltiplos formatos usando Pandas**:\n",
    "# MAGIC    - **JSON**: Formato texto leg√≠vel, ideal para dados brutos (salvo com Pandas)\n",
    "# MAGIC    - **CSV**: Formato texto universal, leg√≠vel por humanos (salvo com Pandas)\n",
    "# MAGIC    - **Parquet**: Formato bin√°rio columnar, otimizado para Big Data (salvo com Pandas)\n",
    "# MAGIC 5. ‚úÖ **Convers√£o Pandas ‚Üí PySpark**: Converte DataFrame Pandas para PySpark\n",
    "# MAGIC 6. ‚úÖ **Delta Table**: Salva no Delta Table usando PySpark (formato com ACID transactions e time travel)\n",
    "# MAGIC 7. ‚úÖ **Convers√£o**: Delta Table ‚Üí Spark DataFrame\n",
    "# MAGIC\n",
    "# MAGIC **Compara√ß√£o de Formatos:**\n",
    "# MAGIC - üìÑ **CSV**: Leg√≠vel, maior, mais lento ‚Üí Use para debugging e dados pequenos\n",
    "# MAGIC - üìä **Parquet**: Bin√°rio, menor, mais r√°pido ‚Üí Use para Big Data e analytics\n",
    "# MAGIC - üéØ **Delta Table**: Parquet + ACID + Time Travel ‚Üí Use para Data Warehouses e pipelines cr√≠ticos\n",
    "# MAGIC\n",
    "# MAGIC **Pr√≥ximos passos**: Criar dashboard e agente de IA para an√°lise dos dados!"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "get_bitcoin_full.py",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
